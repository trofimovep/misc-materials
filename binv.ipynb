{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "from numpy.linalg import inv, pinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_into_blocks(matrix: np.ndarray, *args, verbose=True):\n",
    "    \"\"\"\n",
    "    1. You have to specify what *args can be for statically typed.\n",
    "    2. Instead of setting blocks_amount to None, we can let it be empty.\n",
    "    \"\"\"\n",
    "\n",
    "    msize = matrix.shape\n",
    "    # if verbose is True:\n",
    "    #     print(\"Block Division\")\n",
    "    #     print(\"Matrix size for binv: \")\n",
    "    #     print(msize)\n",
    "\n",
    "    blocks_amount = None if not args else args[0]\n",
    "    if blocks_amount is None or blocks_amount == 0:\n",
    "        blocks_amount = msize[1]\n",
    "    block_size = msize[1] // blocks_amount + (msize[1] % blocks_amount)\n",
    "    blocks = {}\n",
    "    start = 0\n",
    "\n",
    "    # Loops\n",
    "    for b in range(blocks_amount + 1):\n",
    "        end = (start + block_size) - 1\n",
    "        if block_size == 1:\n",
    "            end = start + 1\n",
    "        if end > msize[1] - 1:\n",
    "            end = msize[1]\n",
    "\n",
    "        block = matrix[:, start:end]\n",
    "        if block.size != 0:\n",
    "            blocks[b] = block\n",
    "        start = end\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def pinvblock(block: np.ndarray, block_id: int, pblocks: dict):\n",
    "    pinv_block = pinv(block)\n",
    "    pblocks[block_id] = pinv_block\n",
    "    return pblocks\n",
    "\n",
    "\n",
    "def pinvblocks(blocks: dict) -> dict:\n",
    "    pinvblocks = {}\n",
    "    pool = Pool(len(blocks))\n",
    "    for block_id, block in blocks.items():\n",
    "        pool.apply_async(pinvblock, (block, block_id, pinvblocks))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return pinvblocks\n",
    "\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# C++ potentially\n",
    "def form_row(pblock_id: int, pblock: np.ndarray, blocks: dict, rows: dict):\n",
    "    \"\"\"\n",
    "    The concatenation in the end can be improved because it always allocates new\n",
    "    memory each time it is called. Don't we know the number of rows\n",
    "    in the beginning?\n",
    "    \"\"\"\n",
    "\n",
    "    row = []\n",
    "    for i in range(len(blocks)):\n",
    "        if i == pblock_id:\n",
    "            result = np.eye(pblock.shape[0])\n",
    "        else:\n",
    "            result = np.matmul(pblock, blocks.get(i))\n",
    "        # print(\"result \", type(result), result.shape)\n",
    "        row.append(result)\n",
    "\n",
    "    # Optimize\n",
    "    rows[pblock_id] = np.concatenate((row), axis=1)\n",
    "    # print(\"Final result: \", rows.shape)\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "def build_Pmatrix(blocks: dict, pblocks: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Nr of blocks: Make each block as small as possible\n",
    "    Concenate by allocation before.\n",
    "    Do we parralelize along dim0 or dim1?\n",
    "    \"\"\"\n",
    "    result_rows = {}\n",
    "    pool = Pool(len(blocks))\n",
    "    for id, pblock in pblocks.items():\n",
    "        pool.apply_async(form_row, (id, pblock, blocks, result_rows))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    sorted(result_rows)\n",
    "    rows = list(result_rows.values())\n",
    "    return np.concatenate(rows, axis=0)\n",
    "\n",
    "\n",
    "def binv(matrix: np.ndarray, *args, verbose=True):\n",
    "    blocks = divide_into_blocks(matrix, *args)\n",
    "    pblocks = pinvblocks(blocks)\n",
    "    Pmatrix = build_Pmatrix(blocks, pblocks)\n",
    "    inverted_Pmatrix = inv(Pmatrix)\n",
    "    sorted(pblocks)\n",
    "    pseudoinversed = np.matmul(\n",
    "        inverted_Pmatrix, np.concatenate(list(pblocks.values()), axis=0)\n",
    "    )\n",
    "    return pseudoinversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv, norm\n",
    "\n",
    "\n",
    "def fast_binv(A: np.array, verbose=True):\n",
    "    binvR_array = np.empty(shape=(A.shape[1], A.shape[0]))\n",
    "    for i in range(A.shape[1]):\n",
    "        binvR_array[i, :] = (A[:, i] / norm(A[:, i])).T\n",
    "    inv_matr = np.matmul(binvR_array, A)\n",
    "    inv_matr = inv(inv_matr)\n",
    "    result = np.matmul(inv_matr, binvR_array)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpinv(A: np.array):\n",
    "    return np.matmul(np.linalg.inv(np.matmul(A.T, A)), A.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def super_fast_binv(C, split_index=None, method=\"svd\"):\n",
    "    \"\"\"\n",
    "    TODO: optimize! \n",
    "    \n",
    "    Optimized pseudoinverse of [A | B] using QR/SVD block preprocessing.\n",
    "\n",
    "    Parameters:\n",
    "        C (ndarray): Input matrix of shape (m, n1 + n2)\n",
    "        split_index (int or None): Index to split C into A and B (columns). If None, split in half.\n",
    "        method (str): 'svd' (default) or 'qr'\n",
    "\n",
    "    Returns:\n",
    "        AB_pinv (ndarray): Pseudoinverse of the concatenated matrix [A | B]\n",
    "    \"\"\"\n",
    "    m, n = C.shape\n",
    "    if split_index is None:\n",
    "        split_index = n // 2\n",
    "\n",
    "    A = C[:, :split_index]\n",
    "    B = C[:, split_index:]\n",
    "\n",
    "    n1 = A.shape[1]\n",
    "    n2 = B.shape[1]\n",
    "\n",
    "    if method == \"svd\":\n",
    "        UA, SA, VA_T = np.linalg.svd(A, full_matrices=False)\n",
    "        UB, SB, VB_T = np.linalg.svd(B, full_matrices=False)\n",
    "\n",
    "        RA_inv = VA_T.T * (1.0 / SA)\n",
    "        RB_inv = VB_T.T * (1.0 / SB)\n",
    "\n",
    "        # Pad the R blocks to same column size\n",
    "        RA_inv_padded = np.hstack((RA_inv, np.zeros((n1, n2))))\n",
    "        RB_inv_padded = np.hstack((np.zeros((n2, n1)), RB_inv))\n",
    "\n",
    "    elif method == \"qr\":\n",
    "        UA, RA = np.linalg.qr(A, mode=\"reduced\")\n",
    "        UB, RB = np.linalg.qr(B, mode=\"reduced\")\n",
    "\n",
    "        RA_inv = np.linalg.inv(RA)\n",
    "        RB_inv = np.linalg.inv(RB)\n",
    "\n",
    "        RA_inv_padded = np.hstack((RA_inv, np.zeros((n1, n2))))\n",
    "        RB_inv_padded = np.hstack((np.zeros((n2, n1)), RB_inv))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'svd' or 'qr'.\")\n",
    "\n",
    "    Q_comb = np.hstack((UA, UB))\n",
    "    R_comb = np.vstack((RA_inv_padded, RB_inv_padded))  # shape (n1 + n2, n1 + n2)\n",
    "\n",
    "    AB_pinv = R_comb @ Q_comb.T  # Final pseudoinverse\n",
    "    return AB_pinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "*binv(matrix: np.ndarray, \\*args)* -  the function which returns the Moore-Penrouse pseudo-inversed matrix for full column rank case using block inversion (the formula requires $A^+A=E$). \\\n",
    "Let $A=[A_1 A_2 \\dots A_n]$ - full column rank (block) matrix, then the pseudoinversed matrix can be calculated by \n",
    "$$A^+ = [A_1 A_2 \\dots A_n]^+=\\begin{bmatrix}\n",
    "        \\begin{bmatrix}\n",
    "            A_1^+ \\\\\n",
    "            A_2^+ \\\\\n",
    "            \\dots \\\\\n",
    "            A_n^+ \\\\\n",
    "        \\end{bmatrix}\n",
    "        [A_1 A_2 \\dots A_n]\n",
    "    \\end{bmatrix}^{-1}      \\begin{bmatrix}\n",
    "            A_1^+ \\\\\n",
    "            A_2^+ \\\\\n",
    "            \\dots \\\\\n",
    "            A_n^+ \\\\\n",
    "        \\end{bmatrix}.$$\n",
    "        \n",
    "\n",
    "### Usage\n",
    "By default *binv(matrix: np.ndarray)* receives only matrix. Then it counts that each column is a different block. In that case, there is no advantage and the time of pseudoinversion is equal to the standard function *pinv*. \\\n",
    "**!** For effective calculation pass the amount of blocks in matrix *binv(matrix: np.ndarray, blocks_amount: int)*. Then each block will be calculated in a different thread. \n",
    "### Examples\n",
    "    - binv(np.random.rand(rows, coulmns)) \n",
    "    - binv(np.random.rand(rows, columns), 10)\n",
    "**Note.** To satisfy the full column rank condition there should be $ rows \\geq columns$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
